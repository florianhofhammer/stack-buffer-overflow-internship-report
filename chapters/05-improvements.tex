\chapter{Improvements for defense mechanisms}
\label{chp:defense-mechanism-improvements}

In this chapter, we refer to weaknesses in current security mechanisms that were described in \cref{chp:current-defense-mechanisms,chp:attack-vectors} and present several suggestions on how to improve security in regards to stack buffer overflow exploit prevention.

In \cref{sec:aslr-improvements,sec:stack-canary-improvements} we refer to mechanisms mainly based on randomization and the problems coming with such randomization.
In \cref{sec:function-pointer-protection-improvements,sec:control-flow-integrity} we then present more deterministic approaches to protect processes against stack buffer overflow exploits.
Finally, \cref{sec:performance-comparison} concludes with comparing the performance impact of the previously presented security improvements.

\section{\glsentrylong{aslr} (\glsentryshort{aslr})}
\label{sec:aslr-improvements}

As mentioned in \cref{subsec:address-space-layout-randomization}, \gls{aslr} is performed on a per-process basis.
Thus, addresses are not re-randomized on \texttt{fork} \cite[1\psq]{Lu2016}.
In combination with weak entropy on the randomization \cites{MarcoGisbert2014}[3\psqq]{MarcoGisbert2016}, \gls{aslr} can be bypassed by brute-forcing addresses as a whole or byte by byte (cf. \cref{sec:brute-force}).

As a consequence, several methods to improve the randomization quality of \gls{aslr} were proposed over the years.

\subsection{Randomization granularity}
\label{subsec:randomization-granularity}

In \citeyear{Shacham2004}, \citeauthor{Shacham2004} analyzed the \gls{aslr} functionality of the PaX kernel patch \cite{Shacham2004}.
In this report, we were generally referring to the \gls{aslr} functionality natively included in the Linux kernel.
However, as the PaX patches haven't been updated since Linux kernel 3.6 in 2012 \cite{PaXTeam} and the base functionality is similar, it is possible to transfer suggestions on how to improve PaX \gls{aslr} onto the Linux kernel \gls{aslr}.

An important proposal by \citeauthor{Shacham2004} is to increase the granularity of address randomization.
Instead of just randomizing the base address of code and libraries they suggest to also randomize the order of variables and functions either at compile time or at runtime \cite[303\psq]{Shacham2004}.

\subsubsection{Compile time randomization}
\label{subsubsec:randomization-granularity-compile-time}

\Citeauthor{Shacham2004} suggest increasing randomization granularity already at compile time in the generated binary.
This can be achieved by randomly reordering functions and variables and adjusting references to those functions and variables during compilation and linking \cite[304]{Shacham2004}.

Such a method does not incur any runtime performance overhead, as no additional calculations or instructions have to be computed during runtime.
Any binary of a program compiled with such compile time randomization may have a different layout but contains the same functions and variables.

As an advantage, \citeauthor{Shacham2004} state enhanced entropy of up to additional 12 bits.
On the other hand, if looking at shared libraries, compile time randomization can only provide protection against remote attackers that do not have access to the same library binary.
A local attacker could just reference the same shared library in their own code and thus extract the library's layout \cite[304]{Shacham2004}.
As a consequence, this method is just effective if the binaries in a user base are frequently recompiled so that they are frequently re-randomized.
This means that compile time randomization cannot provide any protection if the program or library is just compiled once per version and distributed in binary format.
In this case, an attacker can obtain the binary in the same version as the target and analyze its layout.
A certain level of protection is therefore just obtained if the executable or library is distributed as source code and compiled by the users themselves or frequently recompiled for distribution even between version changes.

\subsubsection{Runtime randomization}
\label{subsubsec:randomization-granularity-runtime}

Another approach is to reorder functions at runtime.
Usually, function calls within a library or executable use direct relative addressing, as the offsets between functions and instructions remain the same on every instantiation, no matter of the randomized base address.
If this direct relative addressing is changed to indirect relative addressing, the dynamic loader can assign randomized addresses to individual functions by storing the generated address in memory so that other functions referencing the first function can retrieve the location via indirect relative addressing \cite[304\psq]{Shacham2004}.

The advantage of such an approach is that a distributed binary file can be the same for any user but the functions' order and individual addresses can change from instantiation to instantiation.
Thus, an attacker cannot reliably predict functions' addresses by calculating them with known offsets from a known address.
This means that without runtime randomization, an attacker only needs to find the address of a single function in a library and can determine the addresses of any other functions or specific instructions and gadgets by calculating them with the help of known offsets.
With runtime randomization, offsets are randomized because of different function order and even when leaking a single address, other addresses cannot be reliably predicted based on the known value.
This makes it especially hard to leverage code reuse attacks where gadgets are chained, as the individual gadgets' addresses cannot be calculated by some base address or offsets.

However, this approach also has several disadvantages.
Firstly, indirect relative addressing implies memory lookups for getting the relative offsets.
In comparison to direct relative addressing where the offset is encoded in the instruction bytes, this additional memory lookup incurs a performance penalty.
Secondly, randomizing the functions' order from instantiation to instantiation on a granularity finer than page-size (4 kiB, mostly) makes it difficult to share memory pages between processes, as different processes may have different randomized function orders due to the dynamic loader randomizing the order of functions in every process independently.
This problem can be solved by only shuffling page-sized and page-aligned groups of functions at the cost of reducing the granularity.
However, randomization granularity is still higher than without reordering functions at all.
Last, the addresses of functions have to be determined by the dynamic loader.
Usually, this is done by looking addresses up in the \gls{got} of an executable or library.
Reordering functions eliminates fixed offsets used for such addressing and thus requires modifying the \gls{got} concept or replacing it by something else.
Finding a suitable replacement with small to no overhead and performance penalty is difficult \cite[304\psq]{Shacham2004}.

\bigskip\noindent
In conclusion, \citeauthor{Shacham2004} propose several possibilities on how to enhance \gls{aslr}.
Because of the implementation difficulties they also present, they provide no actual implementation or performance analysis.
Still, they refer to \cite{Bhatkar2003} who in their paper \citetitle{Bhatkar2003} obtained an overhead up to 21\% with the performance overhead being below 10\% for most of their analysis of similar approaches \cite[117]{Bhatkar2003}.

\subsection{\glsentryshort{aslr}v3 and \glsentrylong{aslr-ng} (\glsentryshort{aslr-ng})}
\label{subsec:aslrv3-aslr-ng}

\acs{aslr}v3 and \gls{aslr-ng} are both proposals by \citeauthor{MarcoGisbert2014}, the former one presented in their \citeyear{MarcoGisbert2014} paper \citetitle{MarcoGisbert2014}, the latter one in the \citeyear{MarcoGisbert2016} paper \citetitle{MarcoGisbert2016}.
They have similar goals but are working a little bit differently.

\subsubsection{\glsentryshort{aslr}v3}
\label{subsubsec:aslrv3}

\acs{aslr}v3 was developed by \citeauthor{MarcoGisbert2014} to cope with the \emph{offset2lib} weakness in \gls{aslr}.
When a \gls{pie} is loaded, it is loaded in contiguous memory with dynamically loaded libraries.
This allows an attacker who can leak an address from the executable to calculate addresses of libraries, functions in libraries and also gadgets for code reuse attacks in libraries from the leaked address and fixed offsets, hence the name offset2lib.

The \acs{aslr}v3 patch tries to mitigate this weakness by locating the executable at a randomized virtual memory address completely independent from any libraries \cite{MarcoGisbert2014}.
The patch was submitted to the Linux kernel mailing list but wasn't accepted after a long discussion (cf. \cite{MarcoGisbert2014a} and the discussion thread as well as the corresponding source files in the current kernel at \cite{LKD2020}).

\subsubsection{\glsentrylong{aslr-ng} (\glsentryshort{aslr-ng})}
\label{subsubsec:aslr-ng}

\gls{aslr-ng} can be seen as an extension to \acs{aslr}v3.
In fact, the \emph{conservative} operation mode of \gls{aslr-ng} has the same effect as \acs{aslr}v3 by separating the executable from the libraries when randomizing base addresses.
In this mode, the memory address range is divided into four zones, one for each of stack, heap, executable and memory mapped objects such as libraries, memory mapped files and more \cite[11]{MarcoGisbert2016}.
However, \citeauthor{MarcoGisbert2016} propose an additional three operation modes for \gls{aslr-ng}.

The \emph{concentrated} mode randomizes the base address but puts all objects side by side without any spacing.
In this mode, memory space fragmentation is not an issue, as no spaces between the objects occur \cite[11]{MarcoGisbert2016}.

In the \emph{extended} mode, objects' locations in memory are randomized a little bit more as in the conservative mode.
Instead of just randomizing the base addresses of four zones and putting memory objects into those zones, memory objects are grouped based on their type.
This means that memory mapped files are grouped together as well as libraries or thread stacks but they are all placed into memory independently from respective other kinds of memory objects \cite[11]{MarcoGisbert2016}.
This approach basically introduces zones like in the conservative approach with higher granularity.

In the \emph{paranoid} mode, each memory object is placed into memory at a completely random address.
This implies the highest security of all four approaches, as no correlation between memory objects in memory can be established \cite[11]{MarcoGisbert2016}.
However, fragmentation might become an issue, as the objects are randomly spread.
In the worst case, this could mean that even with the huge 64 bit address space a process might not be able to allocate enough memory for its data, as the free space between any two already randomly placed objects is too small.
However, \citeauthor{MarcoGisbert2016} stress that fragmentation is not a big issue on 64 bit systems because of the huge address space in comparison to 32 bit systems where fragmentation could easily become a problem \cite[3]{MarcoGisbert2016}.
Apart from that, they suggest reserving a certain part of the virtual memory for huge \texttt{mmap} requests and by default only put objects randomly in the non-reserved part.
This approach still has higher entropy than default Linux \gls{aslr} or PaX-\gls{aslr} \cite[10\psq]{MarcoGisbert2016} and thus provides additional security.

\bigskip\noindent
Similar to the suggestions by \citeauthor{Shacham2004}, \citeauthor{MarcoGisbert2016} propose finer randomization granularity.
The main difference is that the approaches to enhance randomization granularity proposed by \citeauthor{Shacham2004} are mainly based on sub-page randomization inside of memory objects such as shared libraries whereas \citeauthor{MarcoGisbert2016} try to improve randomization on an object-level granularity by removing or at least minimizing correlations between memory objects.

\subsection{\textsc{RuntimeASLR}}
\label{subsec:runtimeaslr}

\textsc{RuntimeASLR} as proposed by \citeauthor{Lu2016} in \citeyear{Lu2016} aims to prevent information leaking by clone-probing or stack reading as described in \cref{subsec:bf-information-leaking} by re-randomizing addresses on a call to \texttt{fork}.

As the child process is working on an exact copy of the parent process's address space after fork, any addresses and pointers in the child process's address space have to be replaced on re-randomization.
Thus, \textsc{RuntimeASLR} consists of three steps.
Firstly, \emph{taint policies} have to be established in order to find and track pointers in memory.
Tainting means keeping track of data and depending on the operations and the current marking, a value is either marked or unmarked after a certain operation which transforms the data into a pointer or removes the pointer property, respectively.
The taint policies define when to mark or unmark data as a pointer.
Secondly, based on the taint policies generated in the first step, pointers in memory are tracked.
On the one hand, missing a single pointer can lead to program crashes after re-randomization, as it may then point into invalid memory regions and cause memory violations such as a \gls{segfault}.
On the other hand, accidentally marking normal data as a pointer and manipulating it on re-randomization may lead to unexpected results, bugs and errors.
Therefore, it is important that the taint policies are established very carefully and tested thoroughly.
The third and last step is to replace any pointers in memory by their re-randomized counterparts based on the tracked pointers from the second step when forking a child process \cite[3\psq]{Lu2016}.

The first two steps are carried out by binary instrumentation with the Pin framework developed and distributed by Intel \cites{Levi2018}[3\psq]{Lu2016}.
As tracking all pointers and any memory access incurs performance overhead to the executable, Pin is detached in the third step from the child process in order to run the child process with no performance overhead at all.
Thus, only the parent process may experience slowdowns because of pointer tracking \cite[4,11\psqq]{Lu2016}.
This approach implies that child processes forking again will not re-randomize their respective child processes' address spaces and those are thus not protected against clone-probing attacks.
For this reason, a feature to configure which layer of child processes should enable or disable instrumentation for pointer tracking is included in the authors' proposal \cite[9]{Lu2016}.

In their performance measurements, \citeauthor{Lu2016} found that when looking at a forking web server, a maximum overhead of 0.51\% for response time and 0.5\% for throughput was incurred.
They conclude from their measurements that this performance penalty may well be due to measurement inaccuracy and thus no real performance difference is measurable \cite[11\psq]{Lu2016}.
In contrast, when looking at the performance overhead for a single \texttt{fork} call or benchmarking suites such as the \gls{spec} \acs{cpu}2006 benchmark, a performance penalty is very much measurable.
The authors found that a single call to \texttt{fork} with \textsc{RuntimeASLR} takes about 137.5 ms, whereas the original \texttt{fork} is conducted in about 0.1 ms.
Thus, from such a micro-benchmark point of view, \textsc{RuntimeASLR} incurs huge performance penalties to forking processes.
This finding is also supported by the results of the \gls{spec} \acs{cpu}2006 benchmark where runtimes are multiplied by almost 25,000 in the worst case \cite[12]{Lu2016}.

During the performance evaluation and the discussion of their results, \citeauthor{Lu2016} conclude that even those huge performance penalties are negligible for the use-case they developed \textsc{RuntimeASLR} for.
The target for such \gls{aslr} enhancements are forking server daemons that are vulnerable to clone-probing attacks.
As such daemons often have a pre-forking approach, meaning that they already fork child processes during startup and not only on incoming requests and as such daemons usually offload extensive calculations to the child processes which are not subject to performance overhead, only the startup of such a daemon and its child processes may take longer than usual.
As soon as it is running, however, performance overhead is negligible.
Still, the authors conclude that their approach is not suitable for all general-purpose programs, as pointer tracking might incur too much performance overhead in the base process \cite[12\psq]{Lu2016}.


\section{Stack canaries}
\label{sec:stack-canary-improvements}

As shown in \cref{subsec:bf-information-leaking}, stack canaries are highly vulnerable to clone-probing attacks where an attacker might leak the stack canary's value byte by byte.
Thus, several approaches for preventing stack canary bypassing by repeated calls to a process vulnerable to a stack buffer overflow were presented in the past.
Those approaches mainly differ in the frequency of stack canary renewals but they have in common that they try to abandon the current per-process stack canary instantiation method.
In current userspace programs, a stack canary is generated by \gls{glibc} at load time \cite[\texttt{csu/libc-start.c}]{FSF2020a}.
This canary value is then used throughout all functions in the process and across all child processes that do not replace themselves with a new process (for example by calling \texttt{exec}).
The more often the canary value is reused, the bigger the attack surface, as it is sufficient to determine only a single canary value to break the \gls{ssp} protection of all functions in a process and its child, sibling and parent processes if none of them were replaced and re-randomized.

\subsection{\glsentrylong{rafssp} (\glsentryshort{rafssp})}
\label{subsec:renewssp}

The concept of \gls{rafssp} was presented in \citeyear{MarcoGisbert2013} by \citeauthor{MarcoGisbert2013}.
As the name already suggests, the authors propose to regenerate a new stack canary after every \texttt{fork} in order to prevent whole-word or bytewise brute-force attacks.
The former ones are made significantly harder, as an attacker can not just iterate over the whole possible space of canary values and test all possible values until the correct one is found because the value changes from check to check.
The latter ones are made completely impossible, as even finding a correct byte does not yield any helpful information for further calls to the process under attack because the value is re-randomized and the byte found in a previous call thus certainly is not valid anymore.

\Citeauthor{MarcoGisbert2013} assume a threat model similar to the one from \citeauthor{Lu2016} on their \textsc{RuntimeASLR} approach.
This means that they assume a (pre-)forking daemon that has a parent process running only to dispatch requests to child processes.
Thus, the main computational work is done in the child processes.
Those child processes are assumed to never return into functions earlier in the call chain than the function where the \texttt{fork} call was issued.
This can either be because they loop and wait for new instructions from the parent process after finishing working on a request (reused child processes) or because they exit after finishing computation for one request and a new child process is spawned for every request (discarded child processes).
In both cases, they never return to functions earlier in the call chain and thus to stack frames lower on the stack \cite[245\psq]{MarcoGisbert2013}.

Based on this threat model, the actual approach for \gls{rafssp} is simple.
\Citeauthor{MarcoGisbert2013} implemented the \gls{rafssp} approach as both a pre-load shared library effectively overriding the original \texttt{fork} function and a modification to the C library.
Those implementations are both less than 40 \gls{loc} \cite[247]{MarcoGisbert2013}.
In both cases, a normal \texttt{fork} is issued after which the reference stack canary value which is put on the stack in function prologues and checked against in function epilogues is replaced in the child process by a new random value.
This has the effect that in subsequent function calls, the child process uses a new reference \gls{ssp} value and thus a stack canary different from the parent process, child or sibling processes \cite[245\psqq]{MarcoGisbert2013}.
If the child process was to return into functions earlier in the call chain, the process would crash, as the stack canary value in stack frames lower on the stack still contain the old stack canary value.
This old value differs from the newly generated value which is why the stack canary check in a function epilogue would fail.
Therefore, it is important to deploy such protection only in cases like described above in the threat model where the child process does not return into caller functions by either looping over requests or exiting after finishing a request's computational work.

Of course, adding additional instructions and code to be executed when forking a process incurs a performance overhead in comparison to the original implementation.
However, as their code is very lightweight, \citeauthor{MarcoGisbert2013} could not measure any performance overhead apart from statistical variability caused by external factors such as processor scheduling and \gls{os} when benchmarking the Apache web server with some pre-forked child processes and dynamically forked additional child processes \cite[249\psq]{MarcoGisbert2013}.

\subsection{Dynamic canary randomization}
\label{subsec:dynamic-canary-randomization}

\Citeauthor{Hawkins2016} propose a different approach for re-randomizing stack canaries.
Instead of replacing the canary reference value and only using it in subsequent function prologues and epilogues, they suggest replacing the stack canary in any stack frames and as such also for functions earlier in the call chain.
Of course, such an approach is computationally difficult, as it must not miss any canaries during replacement in order to not make the corresponding function crash when its canary is compared to the reference value.
Additionally, it must not replace any values other than canaries with the new value, as this could lead to bugs or information leakage if an adversary can access such a replaced value.
In order to tackle this problem, the authors suggest maintaining a linked list of all the canaries on the stack.
This linking is achieved by embedding an offset into the canary value \cite{Hawkins2016}.

Specifically, part of the canary value is used as an offset to the previous canary on the stack.
This offset always is the difference of the two canaries' addresses so that adding the offset of a canary to its address yields the address of the next canary.
Addition is necessary because the stack is growing downwards in address space so that stack canaries of earlier stack frames are located at higher addresses.
Because some bytes of the canary values on the stack are used for the offset, those bytes also don't have to be randomized in the reference value.
Instead, the reference stack canary in those bytes holds the sum of all the offsets, initialized with \texttt{0x00} bytes on program startup.
This allows for fast iteration over all canaries on the stack.
Starting with the canary of the current stack frame, iteration runs over all canaries by adding the respective offset to the address and thus calculating the address of the canary in the previous stack frame.
On each such addition, the offset is subtracted from the reference canary value's offset bytes and the canary is checked for correctness.
This allows to check all the canaries for integrity during iteration so that the execution of the process could be aborted if tampering on previous canaries is detected.
This fast iteration also allows to replace canary values easily and at the same time keeping the offsets intact to keep the linked list for further iterations \cite{Hawkins2016}.

With this approach, stack canary re-randomization is not only feasible in forking daemon processes as proposed by \citeauthor{MarcoGisbert2013} \cite{MarcoGisbert2013} but also in general-purpose software that returns to earlier stack frames.
Additionally, such re-randomization can be triggered at any point in the execution and not only on forking.
The implementation of the approach by \citeauthor{Hawkins2016} differs from the implementation presented by \citeauthor{MarcoGisbert2013} in that it does not modify a library or override library functions by pre-loading a shared library but that the binary to be protected by stack canaries is statically rewritten.
With the help of a static binary rewriter, the function prologues and epilogues where stack canaries are set up and checked are replaced with custom prologues and epilogues accounting for the extended functionality.
The static rewriter also allows to define criteria on when to re-randomize the stack canary value \cite{Hawkins2016}.

Performance-wise, the performance impact depends on the frequency of rewriting the stack canaries on the stack.
The more often they are rewritten, the higher the performance impact.
Still, the authors suggest that performance overhead in fact is pretty low and that most of the overhead is due to the static rewriting of the binary.
In an example, they look at \emph{bzip2}, a compression and decompression software.
When re-randomizing the \gls{ssp} value on every \texttt{memcpy}, the overhead is 16.08\% with 13.33\% of the overhead being due to the static binary rewriter.
Thus, the actual runtime overhead is at 2.75\%.
Additionally, \citeauthor{Hawkins2016} ran the \gls{spec} \acs{cpu}2006 benchmarks with and without their canary implementation applied.
The results vary strongly between executables and overhead in comparison to the default \gls{ssp} implementation ranges from basically no overhead to a maximum of about 250\% \cite{Hawkins2016}.
In most cases, however, overhead is low to almost nonexistent.
If in addition the overhead from static rewriting is deducted from the total overhead, runtime overhead is seen to be very low.
As mentioned above, runtime overhead mostly depends on re-randomization frequency because the whole linked list has to be modified in that case.
Thus, it is not easily possible to determine performance overhead, as it strongly depends on the user-chosen re-randomization frequency as well as on the statically rewritten binary.

\subsection{Polymorphic canaries}
\label{subsec:polymorphic-canaries}

In \citeyear{Wang2018}, \citeauthor{Wang2018} presented another approach on how to improve the security implications of stack canaries and make them less vulnerable to information leakage through attacks like \hyperref[subsec:bf-information-leaking]{clone-probing}.
The authors conclude that the problem with current stack canary implementations is that leaking a single canary value from the stack reveals the reference canary value stored in the \gls{tls} and thus renders all other canary values on the stack useless.
Measures such as replacing the reference canary value in the \gls{tls} like for example described in \cref{subsec:renewssp,subsec:dynamic-canary-randomization} usually imply that either functions in the call chain that had their stack frames set up before the re-randomization cannot be returned to without failing canary validation or that a list of canaries on the stack has to be managed in order to replace them all.
The \emph{polymorphic canaries} approach aims to overcome such implications by providing a means to keep the reference canary value in the \gls{tls} secret by not copying it to the stack at any time and to derive the actual canary value on the stack from the secret reference canary value in the \gls{tls} \cite[244\psq]{Wang2018}.
They call their \gls{ssp} implementation \emph{\gls{pssp}} \cite[243]{Wang2018}.

\subsubsection{\glsentryshort{pssp} main design}
\label{subsubsec:p-ssp-main-design}

The main idea is to keep the reference value \textbf{C} in the \gls{tls} the same while putting on the stack not a single 8 byte (\texttt{x86\_64}) canary value but two 8 byte values $ C_0,C_1 $ which result in the reference value when applying a \acs{xor} operation, that is $ \textbf{C} = C_0 \oplus C_1 $.
Such a pair $ (C_0, C_1) $ can be generated randomly at any time and is stored in the \gls{tls} as a so-called ``\emph{\acs{tls} shadow canary}'' \cite[246]{Wang2018}.
Instead of copying the actual canary value onto the stack in the function prologue, the concatenation $ C_0 || C_1 $ is copied onto the stack.
When checking the canary value for integrity in the function epilogue, the values on the stack are \acs{xor}ed together and the result is compared with the actual canary reference value in the \gls{tls} \cite[245\psq]{Wang2018}.
Thus, it is easy to deploy a new stack canary value to prevent clone-probing attacks by simply regenerating randomly a pair $ (C_0, C_1) $.
Integrity checks on stack canaries from earlier stack frames still succeed, as not the actual value on the stack is checked for but the \acs{xor}ed value of the two 8 byte values on the stack and the \acs{xor} result is still the same.
For clone-probing prevention, it makes sense to redeploy new \acs{tls} shadow canary values in a child process after calling \texttt{fork}.
However, the main approach as described allows regenerating $ C_0, C_1 $ at any time without breaking integrity checks.

\Citeauthor{Wang2018} also propose three variants of the main \gls{pssp} approach.

\subsubsection{\glsentryshort{pssp} without modifying the \glsentryshort{tls}}
\label{subsubsec:p-ssp-nt}

Firstly, they present the \emph{\acs{pssp}-NT} variant.
In this variation of the original idea, the \acs{tls} shadow canary is omitted, that is the pair $ (C_0, C_1) $ is not stored in the \gls{tls} and copied onto the stack as needed.
Instead, a pair $ C_0, C_1) $ is generated on the fly when calling a \acs{ssp}-protected function and directly put onto the stack.
Thus, the stack canary value $ C_0 || C_1 $ on the stack differs between stack frames and if generated truly randomly only appears more than once in different stack frames with negligible probability.
An advantage of this approach is that it is not necessary to hook into specific function calls such as \texttt{fork} to regenerate the \acs{tls} shadow canary, as it is re-randomized during every function prologue.
A disadvantage, however, is the performance overhead induced by random value generation during every function prologue \cite[247]{Wang2018}.

\subsubsection{Protecting local variables with \glsentryshort{pssp}}
\label{subsubsec:p-ssp-lv}

Secondly, \citeauthor{Wang2018} propose the \emph{\acs{pssp}-LV} variant of \gls{pssp}.
The goal here is to not only protect control flow information such as \gls{sfp} and \gls{rip} but also critical local variables such as function pointers from being overwritten.
Again, this approach does not depend on a pair $ (C_0, C_1) $ being stored in the \gls{tls}.
Instead, a random 8 byte value is pushed onto the stack in front of any variable to protect on the stack, including the control flow information.
For the last canary value to be placed on the stack, the value $ C_j = \textbf{C} \oplus C_0 \oplus C_1 \oplus \cdots \oplus C_{j - 1} $ is calculated.
Thus, when checking the integrity in the function epilogue, a single modified canary value would change the end result and the comparison of the \acs{xor} result $ \bigoplus_{i = 0}^{j}{C_i} $ would differ from \textbf{C}.
Therefore, not only control flow information but also local variables can be protected from being overwritten with \acs{pssp}-LV.
Still, it is important to note that the random value generation incurs a performance overhead and not the local variables' values are protected but it can only be detected that they have been overwritten at the end of the function and only their memory layout is protected \cite[247]{Wang2018}.
Thus, if a protected function pointer is overwritten somewhere in the function and the corresponding function is evaluated before the containing function returns, control flow can already be diverted by an attacker with by misusing the function pointer and manipulation is only detected when the canary integrity is checked.

\subsubsection{Stack canary exposure resilience}
\label{subsubsec:p-ssp-owf}

Lastly, the \emph{\acs{pssp}-OWF} variant is presented where OWF denotes the term ``one-way function''.
The idea is to create an individual nonce $ n $ for every function and putting the nonce as well as a canary $ C $ calculated by $ C = \mathcal{F} \left( ret || n, \textbf{C} \right) $ with $ ret $ being the return address / \gls{rip} on the stack.
$ \mathcal{F} $ here denotes a one-way function such as a hash function or an encryption function \cite[247\psq]{Wang2018}.
As this function has to be evaluated in every function prologue and epilogue in order to check the canary value for integrity, it incurs a significant overhead.
For this reason, \citeauthor{Wang2018} suggest $ \mathcal{F} $ being the \gls{aes} encryption with return address and nonce being encrypted by the canary used as encryption key, as modern processors often provide hardware-accelerated encryption operations and thus low overhead \cite[248,251]{Wang2018}.

The goal behind this approach is to provide return address integrity and stack canary secrecy in a combined function.
With the key (the reference canary value in the \gls{tls}) not being put on the stack at any point in time, an attacker cannot read it from the stack by clone-probing.
\Gls{aes} keeps the key secret, even when the plaintext ($ ret || n $) is known.
As the plaintext changes with every function call because of a changing nonce, the ciphertext and thus the stack canary also changes.
Thus, protection against stack reading is provided.
Even if the stack canary could somehow be bypassed and nonce and return address overwritten, an attacker would have to select \gls{rip} and nonce in such a way that their encryption with the unknown \gls{tls} reference canary value would yield the same ciphertext as the original values.
Without knowledge of the key, such an attack is basically unfeasible \cite[251]{Wang2018}.

\bigskip\noindent
\Citeauthor{Wang2018} implement their approaches in different ways.
The main design can either be implemented as a compiler plugin or via binary instrumentation.
In both cases, a shared library is used for overriding library functions if the original libraries are supposed to be left unmodified.
The implementation as a compiler plugin is not too complicated because the necessary function prologues and epilogues can be directly emitted by the compiler and the compiler can integrate necessary changes to the \gls{tls}, offsets in the resulting binary, stack layout, etc. into the compilation process.
If the shared libraries such as \gls{glibc} are not modified, it is also necessary to create a pre-loaded shared library that hooks into functions such as \texttt{fork} to re-randomize \acs{tls} shadow canaries \cite[248\psq]{Wang2018}.
Implementing \gls{pssp} via binary instrumentation is possible but a little bit more difficult.
The approach here is to replace compiler-generated function prologues and epilogues putting a \gls{ssp} onto the stack and checking its integrity with function prologues and epilogues accounting for \gls{pssp} stack canaries.
Because offsets and addresses in the executable have to remain unchanged to not disrupt execution, the stack layout and binary layout mustn't change.
This implies shortening $ C_0, C_1 $ from 64 bit to 32 bit each so that their concatenation $ C_0 || C_1 $ results in a 64 bit stack canary just like the original stack canary.
Additionally, the prologues and epilogues mustn't change in length.
Thus, because of additional necessary calculations, parts of the integrity check are put into the \texttt{\_\_stack\_chk\_fail} function which originally only is called when the integrity check failed and aborts execution.
Because of this necessity, a shared library is needed to override this function \cite[249\psq]{Wang2018}.
In both cases, compile process modification or binary instrumentation, overriding functions with pre-loaded shared libraries is not possible if the executable is compiled statically.
In such cases, the authors suggest appending custom functions to the executable by binary rewriting and instrumenting the original functions in such a way that they only jump to the custom implementations \cite[250]{Wang2018}.

Implementation of \acs{pssp}-NT and \acs{pssp}-OWF is conducted solely with the help of a compiler plugin.
As they don't rely on a \acs{tls} shadow canary, no change to library functions to re-randomize canaries or account for a changed \gls{tls} layout is necessary.
Thus, only the corresponding function prologues and epilogues have to be emitted by the compiler.
The \acs{pssp}-LV variant, however, is more complicated to implement, as compilers usually reorder variables on the stack for improved efficiency or security and the protecting canaries also have to be taken into account in such cases.
Additionally, a point in time where canary integrity is checked has to be defined in order to prevent control flow diversion before checking the canaries on returning from the function as described \hyperref[subsubsec:p-ssp-lv]{in the paragraph about \acs{pssp}-LV above}.
Those complications have led the authors to postpone \acs{pssp}-LV implementation to future work \cite[250\psq]{Wang2018}.

Concerning the overhead introduced by their implementations, \citeauthor{Wang2018} report an average performance overhead of 0.24\% for compiler based \gls{pssp} and 1.01\% for instrumentation based \gls{pssp}, with a peak performance penalty of about 2.5\% for compiler based \acs{pssp}-NT in a single executable in the \gls{spec} \acs{cpu}2006 benchmark suite \cite[252]{Wang2018}.
When looking at \gls{cpu} cycle overhead, they report a mere 6 \gls{cpu} cycles for function prologue and epilogue of \gls{pssp} and up to 278 and 343 cycles for \acs{pssp}-OWF and \acs{pssp}-NT, respectively.
The overhead for \acs{pssp}-LV depends on the number of variables to protect and thus on the number of random protection values to generate.
As argued by the authors, the overhead for \acs{pssp}-LV is nearly linear in the number of protected variables with their measurements showing an overhead of about 300 \acs{cpu} cycles per random number.
The authors conclude that the overhead for \gls{pssp}, \acs{pssp}-NT and \acs{pssp}-OWF is negligible, as they account for less than 100 nanoseconds on modern \glspl{cpu} \cite[252\psq]{Wang2018}.
This conclusion is supported by performance measurements on web and database servers such as Apache2%
	\footnote{Project website \href{https://httpd.apache.org}{httpd.apache.org}}%
, NGINX%
	\footnote{Project website \href{https://www.nginx.com}{www.nginx.com}}
or MySQL%
	\footnote{Project website \href{https://www.mysql.com}{www.mysql.com}}
which, if at all, show performance overhead not perceivable by a user (less than 100 nanoseconds in the worst case) \cite[252\psq]{Wang2018}.

\section{\glsentrylong{cfi} (\glsentryshort{cfi})}
\label{sec:control-flow-integrity}

\gls{cfi} follows a completely different approach than measures described before, such as \gls{aslr} or stack canaries.
Instead of relying on low probabilities of success as it is the case with address randomization (\gls{aslr}) or random protector values (\gls{ssp} / stack canaries), \gls{cfi} protects a process's control flow deterministically.

In the following two sections, we first present the original \gls{cfi} proposal by \citeauthor{Abadi2005} \cite{Abadi2005}.
We then present a state of the art implementation by the \citeauthor{IntelCorporation2019} which is implemented with hardware support and thus aims to provide highly secured \gls{cfi} measures with low overhead \cite{IntelCorporation2019}.

\subsection{\glsentryshort{cfi} enforcement principle}
\label{subsec:cfi-principle}

In their \citeyear{Abadi2005} paper titled \citetitle{Abadi2005}, \citeauthor{Abadi2005} first present their \gls{cfi} enforcement approach including an implementation and then show further use cases for which their \gls{cfi} enforcement approach can provide an efficient basis.
In this paper, we will focus on their general \gls{cfi} enforcement approach and on its symbiosis with a shadow call stack as described by the authors.

\subsubsection{\glsentryshort{cfi} enforcement main design}
\label{subsubsec:cfi-main-design}

\Gls{cfi} enforcement generally aims to enforce control flow to follow paths in a \gls{cfg} determined before execution whilst neither executing code that is not part of the \gls{cfg} nor diverting control flow to code that is not designated as a valid target in the \gls{cfg}.
In such a \gls{cfg}, nodes are continuous code snippets and edges connect those code snippets in case of branches, function calls, or similar situations.
It is important to differentiate between \emph{static} and \emph{dynamic} targets of an edge in the \gls{cfg}.
Static targets can be for example functions that are invoked by a hard-coded function call in the binary.
As the target address is fixed in the executable binary, there is no possibility for an attacker to divert control flow by changing the target address of such a function call when applying the \gls{wxorx} policy.
Dynamic targets on the other side are determined at runtime.
This may be the case with branches or function calls that are evaluated depending on conditions and where the \texttt{jmp} or \texttt{call} instructions are executed with regard to register or data memory contents instead of having their targets hard-coded into the binary.
Even though the possible targets may be known ahead of time, modification of the target address by an attacker might be possible.
Thus, it is necessary to dynamically check at runtime whether control flow in such cases still follows a predetermined path in the \gls{cfg} \cite[340\psq, 343]{Abadi2005}.

\Citeauthor{Abadi2005} suggest checking whether a target of a control transferring instruction is a valid target in the sense of the \gls{cfg} by introducing labels that are checked before actually transferring control to the target.
If two targets or destinations are equivalent, that is to say that they have the same set of control transferring sources, their respective labels or \glspl{id} are the same.
A label is inserted into the machine code in front of the corresponding control transfer target.
Before transferring control, the destination's \gls{id} is loaded and compared with the \gls{id} encoded in the binary machine code at the source.
If the two \glspl{id} differ, execution is aborted.
Otherwise, the indirect jump or function call is executed as expected.
The same approach applies for function returns.
After each \texttt{call} instruction, an \gls{id} is inserted which is checked before returning from the called function.
It is important to note that the \glspl{id} chosen have to be unique if the corresponding targets are not equivalent and should not collide with \gls{opcode} encoding in the binary \cite[343\psqq]{Abadi2005}.

\todo{Syntax highlighting in assembly code}
\begin{lstlisting}[float=ht,caption={Function call without and with \acs{cfi} enforcement according to the implementation of {\citeauthor{Abadi2005}} (own code based on {\cite[343\psqq]{Abadi2005}})},label={lst:cfi-function-call}]
; Without CFI enforcement
call 0x8(%rbx)              ; call fptr

; With CFI enforcement
mov 0x8(%rbx), %rax         ; load fptr
cmp 0x4(%rax), $0x12345678  ; cmp w/ ID
jne error_label             ; if != fail
call %rax                   ; call fptr
prefetchnta aabbccdd        ; label ID for ret
\end{lstlisting}

\begin{lstlisting}[float=ht,caption={Return from function without and with \acs{cfi} enforcement according to the implementation of {\citeauthor{Abadi2005}} (own code based on {\cite[343\psqq]{Abadi2005}})},label={lst:cfi-function-return}]
; Without CFI enforcement
; Function body here
ret                         ; return

; With CFI enforcement
prefetchnta 12345678        ; label ID for call
; Function body here
mov (%rsp), %rcx            ; load ret address
cmp 0x4(%rcx), $0xaabbccdd  ; cmp w/ ID
jne error_label             ; if != fail
jmp %rcx                    ; jump to ret. addr.
\end{lstlisting}

\Cref{lst:cfi-function-call,lst:cfi-function-return} show how an implementation of \gls{cfi} enforcement could look like.
Before calling a function dynamically, the \gls{id} is checked.
Also, another \gls{id} is checked before returning from the called function.
The 32 bit labels are implemented via the side-effect-free \texttt{prefetchnta} instruction which provides a hint to the \gls{cpu} which data to prefetch into the processor cache.
The 4 byte offset from the address for the comparison (line 6 in \cref{lst:cfi-function-call} and line 9 in \cref{lst:cfi-function-return}) is caused by the 4 byte \gls{opcode} encoding of the \texttt{prefetchnta} instruction \cites[343\psqq]{Abadi2005}[4-406\psq]{IntelCorporation2020}.

Through such checks, control flow following a path from the \gls{cfg} is enforced if the labels are derived from the \gls{cfg} correctly.

\subsubsection{Extending \glsentryshort{cfi} enforcement with a shadow call stack}
\label{subsubsec:cfi-shadow-stack}

As a function may be called by several other functions and thus return to different functions, \gls{cfi} enforcement as described above can just guarantee that the function returns to a valid target in the \gls{cfg}.
However, it may still be the wrong function that is returned to, as the same label for return target checking may occur more than once in different functions.
Therefore, complementing \gls{cfi} enforcement as described before with a \emph{shadow call stack} can help ensuring that the return target is correct.
A shadow call stack here means a second stack complementing the stack where data such as local variables are stored.
The goal is to separate control flow information from data so that overflowing buffers in data cannot modify control flow information.
Thus, control flow information such as return addresses and stack frame pointers are protected from being overflown \cite[348\psqq]{Abadi2005}.

\Citeauthor{Abadi2005} propose using the \texttt{gs} segment register to reference a protected memory segment which holds the shadow stack.
In their implementation, the offset to the top of the stack is stored at the segment base (i.e. at \texttt{\%gs:0x0}).
The return address is put onto the shadow stack before executing a \texttt{call} instruction.
This \texttt{call} instruction still pushes the return address on the default data stack but when returning from the called function, the return address from the data stack is omitted and the return address from the shadow stack is taken instead \cite[348\psq]{Abadi2005}.
This functionality is shown in \cref{lst:cfi-shadow-stack-function-call,lst:cfi-shadow-stack-function-return}.

\begin{lstlisting}[float=ht,caption={Function call without and with \acs{cfi} enforcement and shadow stack usage according to the implementation of {\citeauthor{Abadi2005}} (own code based on {\cite[348\psqq]{Abadi2005}})},label={lst:cfi-shadow-stack-function-call}]
; Without CFI enforcement
      call %rax                   ; call fptr

; With CFI enforcement
      add $0x8, %gs:0x0           ; inc. shadow stack by 8
      mov %gs:0x0, %rcx           ; get stack top offset
      mov LRET, %gs:%rcx          ; push ret. addr. onto stack
      cmp 0x4(%rax), $0x12345678  ; cmp w/ ID
      jne error_label             ; if != fail
      call %rax                   ; call fptr
LRET: ...
\end{lstlisting}

\begin{lstlisting}[float=ht,caption={Return from function without and with \acs{cfi} enforcement and shadow stack usage according to the implementation of {\citeauthor{Abadi2005}} (own code based on {\cite[348\psqq]{Abadi2005}})},label={lst:cfi-shadow-stack-function-return}]
; Without CFI enforcement
; Function body here
ret                   ; return

; With CFI enforcement
prefetchnta 12345678  ; label ID for call
; Function body here
mov %gs:0x0, %rcx     ; get shadow stack top offset
mov %gs:%rcx, %rcx    ; pop ret. addr. from shadow stack
sub $0x8, %gs:0x0     ; dec. shadow stack by 8
add $0x8, %rsp        ; skip ret. addr. on data stack
jmp %rcx              ; jump to ret. addr.
\end{lstlisting}

The security implication of this approach comes from the combination of \gls{cfi} and shadow stack.
While the shadow stack protects backward edges in the \gls{cfg} such as function returns, \gls{cfi} enforcement protects forward edges such as indirect jumps or function calls.
Additionally, \gls{cfi} enforcement not allowing forward jumps to arbitrary locations can prevent an adversary from targeting instructions that modify the shadow stack by referencing the corresponding memory segment.
Thus, the integrity of the backward edges is protected by \gls{cfi} enforcement and the two measures complement each other to protect both forward and backward edges \cite[348\psq]{Abadi2005}.

\bigskip\noindent
\Citeauthor{Abadi2005} implement their \gls{cfi} enforcement approach on the 32 bit Windows \gls{os} via static binary rewriting \cite[345\psq]{Abadi2005}.
However, the examples shown in \cref{lst:cfi-function-call,lst:cfi-function-return,lst:cfi-shadow-stack-function-call,lst:cfi-shadow-stack-function-return} do not rely on instructions or behavior specific to 32 bit Windows so that the code in the listings is ported to 64 bit \texttt{x86\_64} instructions and can also be used on Linux.

Performance impact of the \gls{cfi} enforcement and shadow stack implementations is measured with the \gls{spec} \acs{cpu}2000 benchmark.
With this benchmark suite, an average performance overhead of 16\% for \gls{cfi} enforcement and 21\% for the combination of \gls{cfi} enforcement and shadow stack can be observed.
The authors suggest that such an overhead is tolerable and that the overhead may drop due to optimizations in the instrumentation \cite[346\psq,349\psq]{Abadi2005}.
However, in comparison to security measures presented in previous sections, this overhead is highly significant and thus may hinder successful adoption.

\subsection{Intel \glsentryshort{cet}}
\label{subsec:cfi-intel-cet}

\section{Function pointer protection}
\label{sec:function-pointer-protection-improvements}

\todo[inline]{Only if enough time left $\Rightarrow$ not a lot of specific approaches, most approaches are combination of better randomization (similar to ASLR), secure memory regions (similar to CFI shadow stack)}

\begin{itemize}
	\item{Encryption}
	\item{Mangling}
	\item{Stored in protected memory regions}
\end{itemize}

\section{Performance comparison of presented approaches}
\label{sec:performance-comparison}

\begin{itemize}
	\item{Table with performance overheads}
	\item{Gather information from different papers}
\end{itemize}