\chapter{Improvements for defense mechanisms}
\label{chp:defense-mechanism-improvements}

In this chapter, we refer to weaknesses in current security mechanisms that were described in \cref{chp:current-defense-mechanisms,chp:attack-vectors} and present several suggestions on how to improve security in regards to stack buffer overflow exploit prevention.

In \cref{sec:aslr-improvements,sec:stack-canary-improvements} we refer to mechanisms mainly based on randomization and the problems coming with such randomization.
In \cref{sec:function-pointer-protection-improvements,sec:control-flow-integrity} we then present more deterministic approaches to protect processes against stack buffer overflow exploits.
Finally, \cref{sec:performance-comparison} concludes with comparing the performance impact of the previously presented security improvements.

\section{\glsentrylong{aslr} (\glsentryshort{aslr})}
\label{sec:aslr-improvements}

As mentioned in \cref{subsec:address-space-layout-randomization}, \gls{aslr} is performed on a per-process basis.
Thus, addresses are not re-randomized on \texttt{fork} \cite[1\psq]{Lu2016}.
In combination with weak entropy on the randomization \cites{MarcoGisbert2014}[3\psqq]{MarcoGisbert2016}, \gls{aslr} can be bypassed by brute-forcing addresses as a whole or byte by byte (cf. \cref{sec:brute-force}).

As a consequence, several methods to improve the randomization quality of \gls{aslr} were proposed over the years.

\subsection{Randomization granularity}
\label{subsec:randomization-granularity}

In \citeyear{Shacham2004}, \citeauthor{Shacham2004} analyzed the \gls{aslr} functionality of the PaX kernel patch \cite{Shacham2004}.
In this report, we were generally referring to the \gls{aslr} functionality natively included in the Linux kernel.
However, as the PaX patches haven't been updated since Linux kernel 3.6 in 2012 \cite{PaXTeam} and the base functionality is similar, it is possible to transfer suggestions on how to improve PaX \gls{aslr} onto the Linux kernel \gls{aslr}.

An important proposal of \citeauthor{Shacham2004} is to increase the granularity of address randomization.
Instead of just randomizing the base address of code and libraries they suggest to also randomize the order of variables and functions either at compile time or at runtime \cite[303\psq]{Shacham2004}.

\subsubsection{Compile time randomization}
\label{subsubsec:randomization-granularity-compile-time}

\Citeauthor{Shacham2004} suggest increasing randomization granularity already at compile time in the generated binary.
This can be achieved by randomly reordering functions and variables and adjusting references to those functions and variables during compilation and linking \cite[304]{Shacham2004}.

Such a method does not incur any runtime performance overhead, as no additional calculations or instructions have to be computed during runtime.
Any binary of a program compiled with such compile time randomization may have a different layout but contains the same functions and variables.

As an advantage, \citeauthor{Shacham2004} state enhanced entropy of up to additional 12 bits.
On the other hand, if looking at shared libraries, compile time randomization can only provide protection against remote attackers that do not have access to the same library binary.
A local attacker could just reference the same shared library in their own code and thus extract the library's layout \cite[304]{Shacham2004}.
As a consequence, this method is just effective if the binaries in a user base are frequently recompiled so that they are frequently re-randomized.
This means that compile time randomization cannot provide any protection if the program or library is just compiled once per version and distributed in binary format.
In this case, an attacker can obtain the binary in the same version as the target and analyze its layout.
A certain level of protection is therefore just obtained if the executable or library is distributed as source code and compiled by the users themselves or frequently recompiled for distribution even between version changes.

\subsubsection{Runtime randomization}
\label{subsubsec:randomization-granularity-runtime}

Another approach is to reorder functions at runtime.
Usually, function calls within a library or executable use direct relative addressing, as the offsets between functions and instructions remain the same on every instantiation, no matter of the randomized base address.
If this direct relative addressing is changed to indirect relative addressing, the dynamic loader can assign randomized addresses to individual functions by storing the generated address in memory so that other functions referencing the first function can retrieve the location via indirect relative addressing \cite[304\psq]{Shacham2004}.

The advantage of such an approach is that a distributed binary file can be the same for any user but the functions' order and individual addresses can change from instantiation to instantiation.
Thus, an attacker cannot reliably predict functions' addresses by calculating them with known offsets from a known address.
This means that without runtime randomization, an attacker only needs to find the address of a single function in a library and can determine the addresses of any other functions or specific instructions and gadgets by calculating them with the help of known offsets.
With runtime randomization, offsets are randomized because of different function order and even when leaking a single address, other addresses cannot be reliably predicted based on the known value.
This makes it especially hard to leverage code reuse attacks where gadgets are chained, as the individual gadgets' addresses cannot be calculated by some base address or offsets.

However, this approach also has several disadvantages.
Firstly, indirect relative addressing implies memory lookups for getting the relative offsets.
In comparison to direct relative addressing where the offset is encoded in the instruction bytes, this additional memory lookup incurs a performance penalty.
Secondly, randomizing the functions' order from instantiation to instantiation on a granularity finer than page-size (4 kiB, mostly) makes it difficult to share memory pages between processes, as different processes may have different randomized function orders due to the dynamic loader randomizing the order of functions in every process independently.
This problem can be solved by only shuffling page-sized and page-aligned groups of functions at the cost of reducing the granularity.
However, randomization granularity is still higher than without reordering functions at all.
Last, the addresses of functions have to be determined by the dynamic loader.
Usually, this is done by looking addresses up in the \gls{got} of an executable or library.
Reordering functions eliminates fixed offsets used for such addressing and thus requires modifying the \gls{got} concept or replacing it by something else.
Finding a suitable replacement with small to no overhead and performance penalty is difficult \cite[304\psq]{Shacham2004}.

\bigskip\noindent
In conclusion, \citeauthor{Shacham2004} propose several possibilities on how to enhance \gls{aslr}.
Because of the implementation difficulties they also present, they provide no actual implementation or performance analysis.
Still, they refer to \cite{Bhatkar2003} who in their paper \citetitle{Bhatkar2003} obtained an overhead up to 21\% with the performance overhead being below 10\% for most of their analysis of similar approaches \cite[117]{Bhatkar2003}.

\subsection{\glsentryshort{aslr}v3 and \glsentrylong{aslr-ng} (\glsentryshort{aslr-ng})}
\label{subsec:aslrv3-aslr-ng}

\acs{aslr}v3 and \gls{aslr-ng} are both proposals by \citeauthor{MarcoGisbert2014}, the former one presented in their \citeyear{MarcoGisbert2014} paper \citetitle{MarcoGisbert2014}, the latter one in the \citeyear{MarcoGisbert2016} paper \citetitle{MarcoGisbert2016}.
They have similar goals but are working a little bit differently.

\subsubsection{\glsentryshort{aslr}v3}
\label{subsubsec:aslrv3}

\acs{aslr}v3 was developed by \citeauthor{MarcoGisbert2014} to cope with the \emph{offset2lib} weakness in \gls{aslr}.
When a \gls{pie} is loaded, it is loaded in contiguous memory with dynamically loaded libraries.
This allows an attacker who can leak an address from the executable to calculate addresses of libraries, functions in libraries and also gadgets for code reuse attacks in libraries from the leaked address and fixed offsets, hence the name offset2lib.

The \acs{aslr}v3 patch tries to mitigate this weakness by locating the executable at a randomized virtual memory address completely independent from any libraries \cite{MarcoGisbert2014}.
The patch was submitted to the Linux kernel mailing list but wasn't accepted after a long discussion (cf. \cite{MarcoGisbert2014a} and the discussion thread as well as the corresponding source files in the current kernel at \cite{LKD2020}).

\subsubsection{\glsentrylong{aslr-ng} (\glsentryshort{aslr-ng})}
\label{subsubsec:aslr-ng}

\gls{aslr-ng} can be seen as an extension to \acs{aslr}v3.
In fact, the \emph{conservative} operation mode of \gls{aslr-ng} has the same effect as \acs{aslr}v3 by separating the executable from the libraries when randomizing base addresses.
In this mode, the memory address range is divided into four zones, one for each of stack, heap, executable and memory mapped objects such as libraries, memory mapped files and more \cite[11]{MarcoGisbert2016}.
However, \citeauthor{MarcoGisbert2016} propose an additional three operation modes for \gls{aslr-ng}.

The \emph{concentrated} mode randomizes the base address but puts all objects side by side without any spacing.
In this mode, memory space fragmentation is not an issue, as no spaces between the objects occur \cite[11]{MarcoGisbert2016}.

In the \emph{extended} mode, objects' locations in memory are randomized a little bit more as in the conservative mode.
Instead of just randomizing the base addresses of four zones and putting memory objects into those zones, memory objects are grouped based on their type.
This means that memory mapped files are grouped together as well as libraries or thread stacks but they are all placed into memory independently from respective other kinds of memory objects \cite[11]{MarcoGisbert2016}.
This approach basically introduces zones like in the conservative approach with higher granularity.

In the \emph{paranoid} mode, each memory object is placed into memory at a completely random address.
This implies the highest security of all four approaches, as no correlation between memory objects in memory can be established \cite[11]{MarcoGisbert2016}.
However, fragmentation might become an issue, as the objects are randomly spread.
In the worst case, this could mean that even with the huge 64 bit address space a process might not be able to allocate enough memory for its data, as the free space between any two already randomly placed objects is too small.
However, \citeauthor{MarcoGisbert2016} stress that fragmentation is not a big issue on 64 bit systems because of the huge address space in comparison to 32 bit systems where fragmentation could easily become a problem \cite[3]{MarcoGisbert2016}.
Apart from that, they suggest reserving a certain part of the virtual memory for huge \texttt{mmap} requests and by default only put objects randomly in the non-reserved part.
This approach still has higher entropy than default Linux \gls{aslr} or PaX-\gls{aslr} \cite[10\psq]{MarcoGisbert2016} and thus provides additional security.

\bigskip\noindent
Similar to the suggestions of \citeauthor{Shacham2004}, \citeauthor{MarcoGisbert2016} propose finer randomization granularity.
The main difference is that the approaches to enhance randomization granularity proposed by \citeauthor{Shacham2004} are mainly based on sub-page randomization inside of memory objects such as shared libraries whereas \citeauthor{MarcoGisbert2016} try to improve randomization on an object-level granularity by removing or at least minimizing correlations between memory objects.

\subsection{\textsc{RuntimeASLR}}
\label{subsec:runtimeaslr}

\textsc{RuntimeASLR} as proposed by \citeauthor{Lu2016} in \citeyear{Lu2016} aims to prevent information leaking by clone-probing or stack reading as described in \cref{subsec:bf-information-leaking} by re-randomizing addresses on a call to \texttt{fork}.

As the child process is working on an exact copy of the parent process's address space after fork, any addresses and pointers in the child process's address space have to be replaced on re-randomization.
Thus, \textsc{RuntimeASLR} consists of three steps.
Firstly, \emph{taint policies} have to be established in order to find and track pointers in memory.
Tainting means keeping track of data and depending on the operations and the current marking, a value is either marked or unmarked after a certain operation which transforms the data into a pointer or removes the pointer property, respectively.
The taint policies define when to mark or unmark data as a pointer.
Secondly, based on the taint policies generated in the first step, pointers in memory are tracked.
On the one hand, missing a single pointer can lead to program crashes after re-randomization, as it may then point into invalid memory regions and cause memory violations such as a \gls{segfault}.
On the other hand, accidentally marking normal data as a pointer and manipulating it on re-randomization may lead to unexpected results, bugs and errors.
Therefore, it is important that the taint policies are established very carefully and tested thoroughly.
The third and last step is to replace any pointers in memory by their re-randomized counterparts based on the tracked pointers from the second step when forking a child process \cite[3\psq]{Lu2016}.

The first two steps are carried out by binary instrumentation with the Pin framework developed and distributed by Intel \cites{Levi2018}[3\psq]{Lu2016}.
As tracking all pointers and any memory access incurs performance overhead to the executable, Pin is detached in the third step from the child process in order to run the child process with no performance overhead at all.
Thus, only the parent process may experience slowdowns because of pointer tracking \cite[4,11\psqq]{Lu2016}.
This approach implies that child processes forking again will not re-randomize their respective child processes' address spaces and those are thus not protected against clone-probing attacks.
For this reason, a feature to configure which layer of child processes should enable or disable instrumentation for pointer tracking is included in the authors' proposal \cite[9]{Lu2016}.

In their performance measurements, \citeauthor{Lu2016} found that when looking at a forking web server, a maximum overhead of 0.51\% for response time and 0.5\% for throughput was incurred.
They conclude from their measurements that this performance penalty may well be due to measurement inaccuracy and thus no real performance difference is measurable \cite[11\psq]{Lu2016}.
In contrast, when looking at the performance overhead for a single \texttt{fork} call or benchmarking suites such as the \gls{spec} \acs{cpu}2006 benchmark, a performance penalty is very much measurable.
The authors found that a single call to \texttt{fork} with \textsc{RuntimeASLR} takes about 137.5 ms, whereas the original \texttt{fork} is conducted in about 0.1 ms.
Thus, from such a micro-benchmark point of view, \textsc{RuntimeASLR} incurs huge performance penalties to forking processes.
This finding is also supported by the results of the \gls{spec} \acs{cpu}2006 benchmark where runtimes are multiplied by almost 25,000 in the worst case \cite[12]{Lu2016}.

During the performance evaluation and the discussion of their results, \citeauthor{Lu2016} conclude that even those huge performance penalties are negligible for the use-case they developed \textsc{RuntimeASLR} for.
The target for such \gls{aslr} enhancements are forking server daemons that are vulnerable to clone-probing attacks.
As such daemons often have a pre-forking approach, meaning that they already fork child processes during startup and not only on incoming requests and as such daemons usually offload extensive calculations to the child processes which are not subject to performance overhead, only the startup of such a daemon and its child processes may take longer than usual.
As soon as it is running, however, performance overhead is negligible.
Still, the authors conclude that their approach is not suitable for all general-purpose programs, as pointer tracking might incur too much performance overhead in the base process \cite[12\psq]{Lu2016}.


\section{Stack canaries}
\label{sec:stack-canary-improvements}

As shown in \cref{subsec:bf-information-leaking}, stack canaries are highly vulnerable to clone-probing attacks where an attacker might leak the stack canary's value byte by byte.
Thus, several approaches for preventing stack canary bypassing by repeated calls to a process vulnerable to a stack buffer overflow were presented in the past.
Those approaches mainly differ in the frequency of stack canary renewals but they have in common that they try to abandon the current per-process stack canary instantiation method.
In current userspace programs, a stack canary is generated by \gls{glibc} at load time \cite[\texttt{csu/libc-start.c}]{FSF2020a}.
This canary value is then used throughout all functions in the process and across all child processes that do not replace themselves with a new process (for example by calling \texttt{exec}).
The more often the canary value is reused, the bigger the attack surface, as it is sufficient to determine only a single canary value to break the \gls{ssp} protection of all functions in a process and its child, sibling and parent processes if none of them were replaced and re-randomized.

\subsection{\glsentrylong{rafssp} (\glsentryshort{rafssp})}
\label{subsec:renewssp}

The concept of \gls{rafssp} was presented in \citeyear{MarcoGisbert2013} by \citeauthor{MarcoGisbert2013}.
As the name already suggests, the authors propose to regenerate a new stack canary after every \texttt{fork} in order to prevent whole-word or bytewise brute-force attacks.
The former ones are made significantly harder, as an attacker can not just iterate over the whole possible space of canary values and test all possible values until the correct one is found because the value changes from check to check.
The latter ones are made completely impossible, as even finding a correct byte does not yield any helpful information for further calls to the process under attack because the value is re-randomized and the byte found in a previous call thus certainly is not valid anymore.

\Citeauthor{MarcoGisbert2013} assume a threat model similar to the one from \citeauthor{Lu2016} on their \textsc{RuntimeASLR} approach.
This means that they assume a (pre-)forking daemon that has a parent process running only to dispatch requests to child processes.
Thus, the main computational work is done in the child processes.
Those child processes are assumed to never return into functions earlier in the call chain than the function where the \texttt{fork} call was issued.
This can either be because they loop and wait for new instructions from the parent process after finishing working on a request (reused child processes) or because they exit after finishing computation for one request and a new child process is spawned for every request (discarded child processes).
In both cases, they never return to functions earlier in the call chain and thus to stack frames lower on the stack \cite[245\psq]{MarcoGisbert2013}.

Based on this threat model, the actual approach for \gls{rafssp} is simple.
\Citeauthor{MarcoGisbert2013} implemented the \gls{rafssp} approach as both a pre-load shared library effectively overriding the original \texttt{fork} function and a modification to the C library.
Those implementations are both less than 40 \gls{loc} \cite[247]{MarcoGisbert2013}.
In both cases, a normal \texttt{fork} is issued after which the reference stack canary value which is put on the stack in function prologues and checked against in function epilogues is replaced in the child process by a new random value.
This has the effect that in subsequent function calls, the child process uses a new reference \gls{ssp} value and thus a stack canary different from the parent process, child or sibling processes \cite[245\psqq]{MarcoGisbert2013}.
If the child process was to return into functions earlier in the call chain, the process would crash, as the stack canary value in stack frames lower on the stack still contain the old stack canary value.
This old value differs from the newly generated value which is why the stack canary check in a function epilogue would fail.
Therefore, it is important to deploy such protection only in cases like described above in the threat model where the child process does not return into caller functions by either looping over requests or exiting after finishing a request's computational work.

Of course, adding additional instructions and code to be executed when forking a process incurs a performance overhead in comparison to the original implementation.
However, as their code is very lightweight, \citeauthor{MarcoGisbert2013} could not measure any performance overhead apart from statistical variability caused by external factors such as processor scheduling and \gls{os} when benchmarking the Apache web server with some pre-forked child processes and dynamically forked additional child processes \cite[249\psq]{MarcoGisbert2013}.

\begin{itemize}
	\item{Random canary created on program startup}
	\item{Same canary for all functions $\Rightarrow$ canary disclosure in one function makes all other functions vulnerable}
	\item{Clone-probing attacks: no new randomization on \texttt{fork}}
\end{itemize}

\section{Function pointer protection}
\label{sec:function-pointer-protection-improvements}

\begin{itemize}
	\item{Encryption}
	\item{Mangling}
	\item{Stored in protected memory regions}
\end{itemize}

\section{\glsentrylong{cfi} (\glsentryshort{cfi})}
\label{sec:control-flow-integrity}

\begin{itemize}
	\item{\gls{cfi} => overhead?}
	\item{Intel CET $\Rightarrow$ shadow stack, branch validation}
\end{itemize}

\section{Performance comparison of presented approaches}
\label{sec:performance-comparison}

\begin{itemize}
	\item{Table with performance overheads}
	\item{Gather information from different papers}
\end{itemize}