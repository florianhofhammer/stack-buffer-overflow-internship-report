\chapter{Improvements for defense mechanisms}
\label{chp:defense-mechanism-improvements}

In this chapter, we refer to weaknesses in current security mechanisms that were described in \cref{chp:current-defense-mechanisms,chp:attack-vectors} and present several suggestions on how to improve security in regards to stack buffer overflow exploit prevention.

In \cref{sec:aslr-improvements,sec:stack-canary-improvements} we refer to mechanisms mainly based on randomization and the problems coming with such randomization.
In \cref{sec:function-pointer-protection-improvements,sec:control-flow-integrity} we then present more deterministic approaches to protect processes against stack buffer overflow exploits.
Finally, \cref{sec:performance-comparison} concludes with comparing the performance impact of the previously presented security improvements.

\section{\glsentrylong{aslr} (\glsentryshort{aslr})}
\label{sec:aslr-improvements}

As mentioned in \cref{subsec:address-space-layout-randomization}, \gls{aslr} is performed on a per-process basis.
Thus, addresses are not re-randomized on \texttt{fork} \cite[1\psq]{Lu2016}.
In combination with weak entropy on the randomization \cites{MarcoGisbert2014}[3\psqq]{MarcoGisbert2016}, \gls{aslr} can be bypassed by brute-forcing addresses as a whole or byte for byte (cf. \cref{sec:brute-force}).

As a consequence, several methods to improve the randomization quality of \gls{aslr} were proposed over the years.

\subsection{Randomization granularity}
\label{subsec:randomization-granularity}

In \citeyear{Shacham2004}, \citeauthor{Shacham2004} analyzed the \gls{aslr} functionality of the PaX kernel patch \cite{Shacham2004}.
In this report, we were generally referring to the \gls{aslr} functionality natively included in the Linux kernel.
However, as the PaX patches haven't been updated since Linux kernel 3.6 in 2012 \cite{PaXTeam} and the base functionality is similar, it is possible to transfer suggestions on how to improve PaX \gls{aslr} onto the Linux kernel \gls{aslr}.

An important proposal of \citeauthor{Shacham2004} is to increase the granularity of address randomization.
Instead of just randomizing the base address of code and libraries they suggest to also randomize the order of variables and functions either at compile time or at runtime \cite[303\psq]{Shacham2004}.

\subsubsection{Compile time randomization}
\label{subsubsec:randomization-granularity-compile-time}

\Citeauthor{Shacham2004} suggest increasing randomization granularity already at compile time in the generated binary.
This can be achieved by randomly reordering functions and variables and adjusting references to those functions and variables during compilation and linking \cite[304]{Shacham2004}.

Such a method does not incur any runtime performance overhead, as no additional calculations or instructions have to be computed during runtime.
Any binary of a program compiled with such compile time randomization may have a different layout but contains the same functions and variables.

As an advantage, \citeauthor{Shacham2004} state enhanced entropy of up to additional 12 bits.
On the other hand, if looking at shared libraries, compile time randomization can only provide protection against remote attackers that do not have access to the same library binary.
A local attacker could just reference the same shared library in their own code and thus extract the library's layout \cite[304]{Shacham2004}.
As a consequence, this method is just effective if the binaries in a user base are frequently recompiled so that they are frequently re-randomized.
This means that compile time randomization cannot provide any protection if the program or library is just compiled once per version and distributed in binary format.
In this case, an attacker can obtain the binary in the same version as the target and analyze its layout.
A certain level of protection is therefore just obtained if the executable or library is distributed as source code and compiled by the users themselves or frequently recompiled for distribution even between version changes.

\subsubsection{Runtime randomization}
\label{subsubsec:randomization-granularity-runtime}

Another approach is to reorder functions at runtime.
Usually, function calls within a library or executable use direct relative addressing, as the offsets between functions and instructions remain the same on every instantiation, no matter of the randomized base address.
If this direct relative addressing is changed to indirect relative addressing, the dynamic loader can assign randomized addresses to individual functions by storing the generated address in memory so that other functions referencing the first function can retrieve the location via indirect relative addressing \cite[304\psq]{Shacham2004}.

The advantage of such an approach is that a distributed binary file can be the same for any user but the functions' order and individual addresses can change from instantiation to instantiation.
Thus, an attacker cannot reliably predict functions' addresses by calculating them with known offsets from a known address.
This means that without runtime randomization, an attacker only needs to find the address of a single function in a library and can determine the addresses of any other functions or specific instructions and gadgets by calculating them with the help of known offsets.
With runtime randomization, offsets are randomized because of different function order and even when leaking a single address, other addresses cannot be reliably predicted based on the known value.
This makes it especially hard to leverage code reuse attacks where gadgets are chained, as the individual gadgets' addresses cannot be calculated by some base address or offsets.

However, this approach also has several disadvantages.
Firstly, indirect relative addressing implies memory lookups for getting the relative offsets.
In comparison to direct relative addressing where the offset is encoded in the instruction bytes, this additional memory lookup incurs a performance penalty.
Secondly, randomizing the functions' order from instantiation to instantiation on a granularity finer than page-size (4 kiB, mostly) makes it difficult to share memory pages between processes, as different processes may have different randomized function orders due to the dynamic loader randomizing the order of functions in every process independently.
This problem can be solved by only shuffling page-sized and page-aligned groups of functions at the cost of reducing the granularity.
However, randomization granularity is still higher than without reordering functions at all.
Last, the addresses of functions have to be determined by the dynamic loader.
Usually, this is done by looking addresses up in the \gls{got} of an executable or library.
Reordering functions eliminates fixed offsets used for such addressing and thus requires modifying the \gls{got} concept or replacing it by something else.
Finding a suitable replacement with small to no overhead and performance penalty is difficult \cite[304\psq]{Shacham2004}.

\bigskip\noindent
In conclusion, \citeauthor{Shacham2004} propose several possibilities on how to enhance \gls{aslr}.
Because of the implementation difficulties they also present, they provide no actual implementation or performance analysis.
Still, they refer to \cite{Bhatkar2003} who in their paper \citetitle{Bhatkar2003} obtained an overhead up to 21\% with the performance overhead being below 10\% for most of their analysis of similar approaches \cite[117]{Bhatkar2003}.

\subsection{\glsentryshort{aslr}v3 and \glsentrylong{aslr-ng} (\glsentryshort{aslr-ng})}
\label{subsec:aslrv3-aslr-ng}

\acs{aslr}v3 and \gls{aslr-ng} are both proposals by \citeauthor{MarcoGisbert2014}, the former one presented in their \citeyear{MarcoGisbert2014} paper \citetitle{MarcoGisbert2014}, the latter one in the \citeyear{MarcoGisbert2016} paper \citetitle{MarcoGisbert2016}.
They have similar goals but are working a little bit differently.

\subsubsection{\glsentryshort{aslr}v3}
\label{subsubsec:aslrv3}

\acs{aslr}v3 was developed by \citeauthor{MarcoGisbert2014} to cope with the \emph{offset2lib} weakness in \gls{aslr}.
When a \gls{pie} is loaded, it is loaded in contiguous memory with dynamically loaded libraries.
This allows an attacker who can leak an address from the executable to calculate addresses of libraries, functions in libraries and also gadgets for code reuse attacks in libraries from the leaked address and fixed offsets, hence the name offset2lib.

The \acs{aslr}v3 patch tries to mitigate this weakness by locating the executable at a randomized virtual memory address completely independent from any libraries \cite{MarcoGisbert2014}.
The patch was submitted to the Linux kernel mailing list but wasn't accepted after a long discussion (cf. \cite{MarcoGisbert2014a} and the discussion thread as well as the corresponding source files in the current kernel at \cite{LKD2020}).

\subsubsection{\glsentrylong{aslr-ng} (\glsentryshort{aslr-ng})}
\label{subsubsec:aslr-ng}

\gls{aslr-ng} can be seen as an extension to \acs{aslr}v3.
In fact, the \emph{conservative} operation mode of \gls{aslr-ng} has the same effect as \acs{aslr}v3 by separating the executable from the libraries when randomizing base addresses.
In this mode, the memory address range is divided into four zones, one for each of stack, heap, executable and memory mapped objects such as libraries, memory mapped files and more \cite[11]{MarcoGisbert2016}.
However, \citeauthor{MarcoGisbert2016} propose an additional three operation modes for \gls{aslr-ng}.

The \emph{concentrated} mode randomizes the base address but puts all objects side by side without any spacing.
In this mode, memory space fragmentation is not an issue, as no spaces between the objects occur \cite[11]{MarcoGisbert2016}.

In the \emph{extended} mode, objects' locations in memory are randomized a little bit more as in the conservative mode.
Instead of just randomizing the base addresses of four zones and putting memory objects into those zones, memory objects are grouped based on their type.
This means that memory mapped files are grouped together as well as libraries or thread stacks but they are all placed into memory independently from respective other kinds of memory objects \cite[11]{MarcoGisbert2016}.
This approach basically introduces zones like in the conservative approach with higher granularity.

In the \emph{paranoid} mode, each memory object is placed into memory at a completely random address.
This implies the highest security of all four approaches, as no correlation between memory objects in memory can be established \cite[11]{MarcoGisbert2016}.
However, fragmentation might become an issue, as the objects are randomly spread.
In the worst case, this could mean that even with the huge 64 bit address space a process might not be able to allocate enough memory for its data, as the free space between any two already randomly placed objects is too small.
However, \citeauthor{MarcoGisbert2016} stress that fragmentation is not a big issue on 64 bit systems because of the huge address space in comparison to 32 bit systems where fragmentation could easily become a problem \cite[3]{MarcoGisbert2016}.
Apart from that, they suggest reserving a certain part of the virtual memory for huge \texttt{mmap} requests and by default only put objects randomly in the non-reserved part.
This approach still has higher entropy than default Linux \gls{aslr} or PaX-\gls{aslr} \cite[10\psq]{MarcoGisbert2016} and thus provides additional security.

\bigskip\noindent
Similar to the suggestions of \citeauthor{Shacham2004}, \citeauthor{MarcoGisbert2016} propose finer randomization granularity.
The main difference is that the approaches to enhance randomization granularity proposed by \citeauthor{Shacham2004} are mainly based on sub-page randomization inside of memory objects such as shared libraries whereas \citeauthor{MarcoGisbert2016} try to improve randomization on an object-level granularity by removing or at least minimizing correlations between memory objects.

\begin{itemize}
	\item{Randomized on program startup}
	\item{Clone-probing attacks: no new randomization on \texttt{fork}}
\end{itemize}

\section{Stack canaries}
\label{sec:stack-canary-improvements}

\begin{itemize}
	\item{Random canary created on program startup}
	\item{Same canary for all functions $\Rightarrow$ canary disclosure in one function makes all other functions vulnerable}
	\item{Clone-probing attacks: no new randomization on \texttt{fork}}
\end{itemize}

\section{Function pointer protection}
\label{sec:function-pointer-protection-improvements}

\begin{itemize}
	\item{Encryption}
	\item{Mangling}
	\item{Stored in protected memory regions}
\end{itemize}

\section{\glsentrylong{cfi} (\glsentryshort{cfi})}
\label{sec:control-flow-integrity}

\begin{itemize}
	\item{\gls{cfi} => overhead?}
	\item{Intel CET $\Rightarrow$ shadow stack, branch validation}
\end{itemize}

\section{Performance comparison of presented approaches}
\label{sec:performance-comparison}

\begin{itemize}
	\item{Table with performance overheads}
	\item{Gather information from different papers}
\end{itemize}